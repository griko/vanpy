input_dir: 'speech_examples_small'

preprocessing:
  file_mapper:
  wav_converter:
    output_dir: 'convert_preprocessed'
    ab: '256k'  # bitrate
    ac: 1       # number of channels
    ar: '16k'   # sample frequency
  ina_speech_segmenter:
    output_dir: 'ina_preprocessed'
    filtered_dir: 'ina_filtered'
    vad_engine: 'sm'
  pyannote_vad:
    output_dir: 'pyannote_vad_preprocessed'
    onset: 0.85             # onset activation threshold
    offset: 0.8             # offset activation threshold
    min_duration_on: 0.1    # remove speech regions shorter than that many seconds
    min_duration_off: 0.0   # fill non-speech regions shorter than that many seconds
  silero_vad:
    output_dir: 'silero_vad_preprocessed'
    sampling_rate: 16000

feature_extraction:
  pyannote_embedding:
    sliding_window_duration: 3.0
    sliding_window_step: 1.0
  speechbrain_embedding:

segment_classifier:
  common_voices_gender:
    pretrained_models_dir: 'pretrained_models/common_voice'
    classification_column_name: 'common_voices_gender_classification'
    verbal_labels: true

  common_voices_age:
    pretrained_models_dir: 'pretrained_models/common_voice'
    classification_column_name: 'common_voices_age_classification'
    verbal_labels: true

  speech_brain_iemocap_emotion:
    pretrained_models_dir: 'pretrained_models/speech_brain_iemocap_emotion'
    classification_column_name: 'speech_brain_iemocap_emotion'
    verbal_labels: true
