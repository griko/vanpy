# -*- coding: utf-8 -*-
"""voice_characterization_framework.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X5GkZ8d3RPV-9tA4-ph3r3yaWsxulUS3
"""

!pip install xgboost==1.5.0

"""**RYou may be asked to restart the runtime after installing xgboost!**"""

import pandas as pd
import numpy as np
from typing import List, Tuple
import os
from tqdm import tqdm
import torch
import time

def create_dirs_if_not_exist(*args:str) -> None:
    for arg in args:
        os.makedirs(arg, exist_ok=True)

create_dirs_if_not_exist('external_libs')

if torch.cuda.is_available():  
  dev = "cuda" 
else:  
  dev = "cpu"  
print(f'running on {dev}')
device = torch.device(dev)

#@title <font size="5"><i>Installing ffmpeg</font> { vertical-output: true }
from IPython.display import clear_output
import os, urllib.request
HOME = os.path.expanduser("~")
pathDoneCMD = f'{HOME}/doneCMD.sh'
if not os.path.exists(f"{HOME}/.ipython/ttmg.py"):
    hCode = "https://raw.githubusercontent.com/yunooooo/gcct/master/res/ttmg.py"
    urllib.request.urlretrieve(hCode, f"{HOME}/.ipython/ttmg.py")

from ttmg import (
    loadingAn,
    textAn,
)

loadingAn(name="lds")
textAn("Installing Dependencies...", ty='twg')
os.system('pip install git+git://github.com/AWConant/jikanpy.git')
os.system('add-apt-repository -y ppa:jonathonf/ffmpeg-4')
os.system('apt-get update')
os.system('apt install mediainfo')
os.system('apt-get install ffmpeg')
clear_output()
print('Installation finished.')

def cut_by_segments(input_path:str, output_path:str, segments_list:List[Tuple[float,float]]) -> None:
    create_dirs_if_not_exist(output_path)
    if segments_list:
      for i, segment in enumerate(segments_list):
          start, stop = segment
          f = ''.join(str(input_path).split("/")[-1].split(".")[:-1])
          subprocess.run(["ffmpeg", "-hide_banner", "-loglevel", "error", "-ss", f"{start}", "-to", f"{stop}", "-y", "-i",
                          f"{input_path}", "-ab", "256k", "-ac", "1", "-ar", "16k", f'{output_path}/{f}_{i}.wav', '-dn', '-ignore_unknown', '-sn'])

"""## Loading audio samples

"""

import IPython.display as ipd
import librosa
import subprocess
import os

input_audio_folder = 'speech_examples'
create_dirs_if_not_exist(input_audio_folder)

"""Please place some files in **speech_exemples** dir"""

def get_audio_files_paths(folder: str, extenstion: str) -> List[str]:
  folder_files = os.listdir(folder)
  return [f'{folder}/{f}' for f in folder_files if f.endswith(extenstion) and os.path.isfile(f'{folder}/{f}')]

# converting mp3 to wav
def convert_mp3_to_wav_16hz_1channel(paths_list: List[str], output_path: str = '') -> None:
  if not paths_list:
    print('You\'ve supplied an empty list to convert')
    return

  for f in paths_list:
    filename = ''.join(f.split("/")[-1].split(".")[:-1])
    if not output_path:
      input_path = ''.join(f.split("/")[:-1])
      output_path = input_path
    subprocess.run(["ffmpeg", "-hide_banner", "-loglevel", "error", "-i",
                        f"{f}", "-ab", "256k", "-ac", "1", "-ar", "16k", f'{output_path}/{filename}.wav', '-dn', '-ignore_unknown', '-sn'])

# input_audio_files_list_mp3 = get_audio_files_paths(input_audio_folder, '.mp3')
# convert_mp3_to_wav_16hz_1channel(input_audio_files_list_mp3, input_audio_folder)  # place wavs in the same dir where mp3s were

## getting only wav files
input_audio_files_list = get_audio_files_paths(input_audio_folder, '')
if not input_audio_files_list:
  print(f'Please place some files in the "{input_audio_folder}" folder', file=sys.stderr)

from IPython.display import Audio, display
# loading with librosa - hardly deals with mp3s and large files
# x, sr = librosa.load(input_audio_files_list[0])
# ipd.Audio(x, rate=sr)

# displaing with IPython.display
display(Audio(input_audio_files_list[0]))

"""## Preprocessing

### Segmenting with Voice Activity Detection (VAD)
"""

# converting mp3 to wav
input_audio_files_list_mp3 = get_audio_files_paths(input_audio_folder, '.mp3')
convert_mp3_to_wav_16hz_1channel(input_audio_files_list_mp3, input_audio_folder)  # place wavs in the same dir where mp3s were

## getting only wav files
input_audio_files_list_wav = get_audio_files_paths(input_audio_folder, '.wav')
print(f'The number of "wav" input files is {len(input_audio_files_list_wav)}')

"""#### INA Speech Segmenter

(INA-speech-segmenter is not compatible with pyannote 2.0. If you've ran pyannote 2.0 vad/embedding, the runtime has to be restarted)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture --no-stderr
# ina_output = 'ina_processed'
# ina_filtered ='ina_filtered'
# create_dirs_if_not_exist(ina_output, ina_filtered, '/content/external_libs/ina')
# !git clone https://github.com/ina-foss/inaSpeechSegmenter.git /content/external_libs/ina
# !cd /content/external_libs/ina/inaSpeechSegmenter
# !pip install .
# !pip install pyannote.algorithms
# !pip install pytextgrid
# !cd /content

from external_libs.ina.inaSpeechSegmenter import Segmenter
import subprocess
seg = Segmenter(vad_engine='sm')

def get_voice_segments(segmentation):
  voice_sections, filtered_sections = [], []
  for s in segmentation:
    kind, start, stop = s
    if kind == 'female' or kind == 'male':
      voice_sections.append((start, stop))
    else:
      filtered_sections.append((start, stop))
  return voice_sections, filtered_sections

for f in tqdm(input_audio_files_list_wav):
  try:
    start = time.time()
    segmentation = seg(f)
    v_segments, f_segments = get_voice_segments(segmentation)
    cut_by_segments(f, ina_output, v_segments)
    cut_by_segments(f, ina_filtered, f_segments)
    end = time.time()
    print(f'Extracted {len(v_segments)} from {f} in {end - start} seconds')
  except AssertionError as err:
    print(f"Error reading {f} with ffmpeg")

"""#### Using pyannote 2.0

(Pyannote 2.0 is not compatible with INA-speech-segmenter. If you've ran INA-speech-segmenter, the runtime has to be restarted
If you've ran it, the runtime has to be restarted)

It is advised to run pyannote 2.0 after INA-speech-segmenter, otherwise pyannote 2.0 will extract lyrics of songs as well
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture --no-stderr
# # !rm -rf pyannote_processed
# pyannote_output = 'pyannote_processed'
# create_dirs_if_not_exist(pyannote_output, '/content/external_libs/pyannote')
# !cd /content/external_libs/pyannote
# !pip install https://github.com/pyannote/pyannote-audio/archive/develop.zip
# !cd /content

from pyannote.audio import Inference
from pyannote.audio.pipelines import VoiceActivityDetection
pipeline = VoiceActivityDetection(segmentation="pyannote/segmentation")
HYPER_PARAMETERS = {  # mean of VAD from cluster_0_stream_xJqJaIPo_20211110_23_38_41_17_0.wav
  # onset/offset activation thresholds
  "onset": 0.85, "offset": 0.8,
  # remove speech regions shorter than that many seconds.
  "min_duration_on": 0.1,
  # fill non-speech regions shorter than that many seconds.
  "min_duration_off": 0
}
pipeline.instantiate(HYPER_PARAMETERS)

# getting INA's preprocessed output
ina_audio_files_list_wav = get_audio_files_paths(ina_output, '.wav')

def get_segments(file):
  sections = []
  try:
    vad = pipeline(file)
    for i, v in enumerate(vad.itersegments()):
      start, stop = v
      sections.append((start, stop))
  except RuntimeError:
    print(f"Could not create VAD pipline for {file} with pyannote")
    if file.endswith('.mp3'):
      print(f"Try to recode with ffmpeg to wav format")
  return sections


for f in tqdm(ina_audio_files_list_wav):
  try:
    start = time.time()
    v_segments = get_segments(f)
    cut_by_segments(f, pyannote_output, v_segments)
    end = time.time()
    print(f'Extracted {len(v_segments)} from {f} in {end - start} seconds')
  except AssertionError as err:
    print(f"Error reading {f} with ffmpeg")

"""#### Using silero-vad

Silero is not extracting music's lyrics, but sometimes misses the promotionals with background sound. Also, it is the fastest VAD comared to pyannote 2.0 and INA.
"""

silero_output = 'silero_processed'
create_dirs_if_not_exist(silero_output)

import torch
from IPython.display import Audio
from pprint import pprint

torch.set_num_threads(1)
model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=False)

(get_speech_timestamps,
 save_audio,
 read_audio,
 VADIterator,
 collect_chunks) = utils

SAMPLING_RATE = 16000
for f in tqdm(input_audio_files_list_wav):
  start = time.time()
  wav = read_audio(f, sampling_rate=SAMPLING_RATE)
  # get speech timestamps from full audio file
  v_segments = [(x['start']/SAMPLING_RATE, x['end']/SAMPLING_RATE) for x in get_speech_timestamps(wav, model, sampling_rate=SAMPLING_RATE)]
  cut_by_segments(f, silero_output, v_segments)
  end = time.time()
  print(f'Extracted {len(v_segments)} from {f} in {end - start} seconds')
  # pprint(speech_timestamps)

"""### Seperating voice from background sounds

#### Using spleeter
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture --no-stderr
# !pip install spleeter
# spleeter_output = 'spleeter_processed'
# spleeter_tmp = f'{spleeter_output}/tmp'
# spleeter_vocals = f'{spleeter_output}/vocals'
# create_dirs_if_not_exist(spleeter_output, spleeter_tmp, spleeter_vocals)
# # !spleeter separate --help

# getting pyannote 2.0's preprocessed output
pyannote_audio_files_list_wav = get_audio_files_paths(pyannote_output, '.wav')

for f in tqdm(pyannote_audio_files_list_wav):
  start = time.time()
  !spleeter separate -p spleeter:2stems-16kHz -o $spleeter_tmp/ $f
  end = time.time()
  print(f'Seperated voice from {f} in {end - start} seconds')

import shutil

# collecting only voicals from the tmp split directory
for dir_path, dir_name in [(f'{spleeter_tmp}/{f}', f) for f in os.listdir(spleeter_tmp) if os.path.isdir(f'{spleeter_tmp}/{f}')]:
  try:
    os.replace(f'{dir_path}/vocals.wav', f'{spleeter_vocals}/{dir_name}.wav ')
  except FileNotFoundError:
    print(f'No "vocals.wav" in {dir_path} was found')

"""### Embedding

#### Using pyannote
"""

from pyannote.audio import Inference
inference_emb = Inference("pyannote/embedding", 
                      window="sliding",
                      duration=3.0, step=1.0)

# getting pyannote 2.0's preprocessed output
pyannote_audio_files_list_wav = get_audio_files_paths(pyannote_output, '.wav')

df = pd.DataFrame()
for f in tqdm(pyannote_audio_files_list_wav):
  try:
    embedding = inference_emb(f)
    tmp = pd.DataFrame(np.mean(embedding, axis=0)).T
    tmp['filename'] = f.split("/")[-1]
    df = pd.concat([tmp, df], ignore_index=True)
  except RuntimeError as e:
    print(f'An error occured in {f}: {e}')

df_pyannote = df
X_test_pyannote = df_pyannote.drop(['filename'], axis=1)
y_test_pyannote = df_pyannote[['filename']]

"""#### Using SpeechBrain"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install speechbrain
# !pip install torchaudio

import torchaudio
from speechbrain.pretrained import EncoderClassifier
classifier = EncoderClassifier.from_hparams(source="speechbrain/spkrec-xvect-voxceleb", savedir="pretrained_models/spkrec-xvect-voxceleb")

# getting pyannote 2.0's preprocessed output
pyannote_audio_files_list_wav = get_audio_files_paths(pyannote_output, '.wav')

df = pd.DataFrame()
for f in tqdm(pyannote_audio_files_list_wav):
  try:
    signal, fs = torchaudio.load(f)
    embedding = classifier.encode_batch(signal)
    tmp = pd.DataFrame(embedding.to('cpu').numpy().ravel()).T
    tmp['filename'] = f.split("/")[-1]
    df = pd.concat([tmp, df], ignore_index=True)
  except RuntimeError as e:
    print(f'An error occured in {f}: {e}')

df

df.drop(['filename'], axis=1).drop_duplicates()

"""## Classification

### Age
"""

import pickle

# load models
file_name = "pretrained_models/common_voices/xgb_us_age_512_model.pkl"
xgb_cl = pickle.load(open(file_name, "rb"))
file_name = "pretrained_models/common_voices/xgb_us_age_512_full_processor.pkl"
full_processor = pickle.load(open(file_name, "rb"))
file_name = "pretrained_models/common_voices/xgb_us_age_512_simple_imputer.pkl"
simple_imputer = pickle.load(open(file_name, "rb"))

X_test_pyannote.columns = X_test_pyannote.columns.astype(str)
X_test = full_processor.transform(X_test_pyannote)
y_test = simple_imputer.transform(
    y_test_pyannote.values.reshape(-1, 1)
)
y_pred = xgb_cl.predict(X_test)
y_pred

"""### Gender"""

import pickle

# load models
file_name = "pretrained_models/common_voices/xgb_gender_512_model.pkl"
xgb_cl = pickle.load(open(file_name, "rb"))
file_name = "pretrained_models/common_voices/xgb_gender_512_full_processor.pkl"
full_processor = pickle.load(open(file_name, "rb"))
file_name = "pretrained_models/common_voices/xgb_gender_512_simple_imputer.pkl"
simple_imputer = pickle.load(open(file_name, "rb"))

X_test_pyannote.columns = X_test_pyannote.columns.astype(str)
X_test = full_processor.transform(X_test_pyannote)
y_test = simple_imputer.transform(
    y_test_pyannote.values.reshape(-1, 1)
)
y_pred = xgb_cl.predict(X_test)
y_pred

"""### Emotion Recognition (Speechbrain pretrained on RAVDESS and TESS)"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture --no-stderr
# !pip install transformers

from transformers import Wav2Vec2Model
from speechbrain.pretrained.interfaces import foreign_class
classifier = foreign_class(source="speechbrain/emotion-recognition-wav2vec2-IEMOCAP", pymodule_file="custom_interface.py", classname="CustomEncoderWav2vec2Classifier")


# getting pyannote 2.0's preprocessed output
pyannote_audio_files_list_wav = get_audio_files_paths(pyannote_output, '.wav')

y_pred = []
for f in tqdm(pyannote_audio_files_list_wav):
  try:
    out_prob, score, index, text_lab = classifier.classify_file(f)
    y_pred.append(text_lab)
  except RuntimeError as e:
    print(f'An error occured in {f}: {e}')

# print(text_lab)
y_pred

"""## Speech to text"""

from time import time
import sys
def save_to_file(text, filename):
  with open(filename, 'w') as file:
    file.write(text)

speech_transcriptions_path = 'speech_transcriptions'
create_dirs_if_not_exist(speech_transcriptions_path)

"""### SpeechRecognition - Google API"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture --no-stderr
# !pip install SpeechRecognition

#import library
import speech_recognition as sr

# Initialize recognizer class (for recognizing the speech)
r = sr.Recognizer()
file_sub = "sr"

# getting pyannote 2.0's preprocessed output
pyannote_audio_files_list_wav = get_audio_files_paths(pyannote_output, '.wav')

# Reading Audio file as source
# listening the audio file and store in audio_text variable
for f in pyannote_audio_files_list_wav:
  with sr.AudioFile(f) as source:
    audio_text = r.listen(source)
    # recoginize_() method will throw a request error if the API is unreachable, hence using exception handling
    try:
      print(f'Transcribing {f}...')
      inference_start = time()
      # using google speech recognition
      transcription = r.recognize_google(audio_text)
      inference_end = time() - inference_start
      print(transcription)
      save_to_file(transcription, f'{speech_transcriptions_path}/{file_sub}_{f.split("/")[-1].split(".")[0]}.txt')
      print('Inference took %0.3fs for %0.3fs audio file.\n' % (inference_end, source.DURATION), file=sys.stdout)
    except Exception as e: print(e)
        # print('API error')

"""### Wav2Vec2"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture --no-stderr
# !pip install -q transformers
# !pip install librosa
# !pip install torch

import librosa
import torch
from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer

tokenizer = Wav2Vec2Tokenizer.from_pretrained("facebook/wav2vec2-base-960h")
model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")

file_sub = "wav2vec"

# getting pyannote 2.0's preprocessed output
pyannote_audio_files_list_wav = get_audio_files_paths(pyannote_output, '.wav')

# Reading Audio file as source
# listening the audio file and store in audio_text variable
for f in pyannote_audio_files_list_wav:
  try:
    print(f'Transcribing {f}...')
    inference_start = time()
    # Loading the audio file
    audio, rate = librosa.load(f, sr = 16000)
    # Taking an input value
    input_values = tokenizer(audio, return_tensors = "pt").input_values
    # Storing logits (non-normalized prediction values)
    logits = model(input_values).logits
    # Storing predicted ids
    prediction = torch.argmax(logits, dim = -1)
    # Passing the prediction to the tokenzer decode to get the transcription
    transcription = tokenizer.batch_decode(prediction)[0]
    inference_end = time() - inference_start
    print(transcription)
    save_to_file(transcription, f'{speech_transcriptions_path}/{file_sub}_{f.split("/")[-1].split(".")[0]}.txt')
    print('Inference took %0.3fs for %0.3fs audio file.\n' % (inference_end, librosa.get_duration(audio, rate)), file=sys.stdout)
  except Exception as e: print(e)
      # print('API error')

"""### Deep Speech by Mozilla"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture --no-stderr
# !pip install deepspeech

deepspeech_model_path = 'pretrained_models/deepspeech'
create_dirs_if_not_exist(deepspeech_model_path)
!wget https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/deepspeech-0.9.3-models.pbmm -P $deepspeech_model_path
!wget https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/deepspeech-0.9.3-models.scorer -P $deepspeech_model_path

from deepspeech import Model, version
import wave
import shlex

model = deepspeech_model_path + '/deepspeech-0.9.3-models.pbmm'
ds = Model(model)
desired_sample_rate = ds.sampleRate()
scorer = deepspeech_model_path + '/deepspeech-0.9.3-models.scorer'
ds.enableExternalScorer(scorer)

file_sub = "ds"

# getting pyannote 2.0's preprocessed output
pyannote_audio_files_list_wav = get_audio_files_paths(pyannote_output, '.wav')

# Reading Audio file as source
# listening the audio file and store in audio_text variable
for f in pyannote_audio_files_list_wav:
  try:
    print(f'Transcribing {f}...')
    inference_start = time()

    fin = wave.open(f, 'rb')
    fs_orig = fin.getframerate()
    audio = np.frombuffer(fin.readframes(fin.getnframes()), np.int16)
    audio_length = fin.getnframes() * (1/fs_orig)
    fin.close()

    transcription = ds.stt(audio)

    inference_end = time() - inference_start
    print(transcription)
    save_to_file(transcription, f'{speech_transcriptions_path}/{file_sub}_{f.split("/")[-1].split(".")[0]}.txt')
    print('Inference took %0.3fs for %0.3fs audio file.\n' % (inference_end, audio_length), file=sys.stdout)
  except Exception as e: print(e)
      # print('API error')

"""### SpeechBrain"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture --no-stderr
# !pip install speechbrain
# !pip install transformers

import speechbrain as sb
from speechbrain.dataio.dataio import read_audio
from IPython.display import Audio
from speechbrain.pretrained import EncoderDecoderASR

asr_model = EncoderDecoderASR.from_hparams(source="speechbrain/asr-crdnn-rnnlm-librispeech", savedir="pretrained_models/asr-crdnn-rnnlm-librispeech")

file_sub = "ds"

# getting pyannote 2.0's preprocessed output
pyannote_audio_files_list_wav = get_audio_files_paths(pyannote_output, '.wav')

# Reading Audio file as source
# listening the audio file and store in audio_text variable
for f in pyannote_audio_files_list_wav:
  try:
    print(f'Transcribing {f}...')
    inference_start = time()

    transcription = asr_model.transcribe_file(f)

    inference_end = time() - inference_start
    print(transcription)
    save_to_file(transcription, f'{speech_transcriptions_path}/{file_sub}_{f.split("/")[-1].split(".")[0]}.txt')
    print('Inference took %0.3fs for XXfs audio file.\n' % (inference_end), file=sys.stdout)
  except Exception as e: print(e)

!pip list