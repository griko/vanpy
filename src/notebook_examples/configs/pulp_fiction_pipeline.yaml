input_dir: '../cut_preprocessed'  # 'speech_examples', 'speech_examples_small'
segment_name_separator: "_"
intermediate_payload_path: 'results/pulp_fiction'
# device: 'cpu'  # 'cpu'/'cuda'
huggingface_ACCESS_TOKEN: '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'
max_workers: 1  # set the number of workers for parallel threads
sampling_rate: 16000
latent_logger:
  enabled: true
  log_each_x_records: 1000

preprocessing:
  file_mapper:
    save_payload: false  # use it if you want to save the payload when the component process is completed
    load_payload: false  # use it if you want to load a previously saved payload, overrides listing of an input_path
#    load_df_path: '{{intermediate_payload_path}}/preprocessing_silero_vad_df_20230703143304_final.csv'
#    load_meta_path: '{{intermediate_payload_path}}/preprocessing_silero_vad_metadata_20230703143304_final.pickle'
    load_df_path: '{{intermediate_payload_path}}/final_feature_classification_1190_df.csv'
    load_meta_path: '{{intermediate_payload_path}}/final_feature_classification_1190_df.pickle'
  wav_converter:
    output_dir: 'convert_preprocessed'
    ab: '256k'  # bitrate, may not be considered depending on a chosen codec
    ac: 1       # number of channels
    ar: '{{sampling_rate}}'   # sample frequency
    acodec: 'pcm_s16le'  # specify codec
    use_dir_name_as_prefix: false  # will name converted file with parent_dir prefix:
                                  # '<path>/<parent_dir>/<filename>' -> '<ouput_dir>/<parent_dir>_<filename>'
    overwrite: false
    save_payload: true
  wav_splitter:
    output_dir: 'split_preprocessed'
#    max_audio_length: 30  # split duration in seconds (overrides max_wav_file_size)
    max_wav_file_size: 1  # split file size in MB  (at least one of max_audio_length or max_wav_file_size should be set)
    overwrite: false
    save_payload: true
  ina_speech_segmenter:
    output_dir: 'ina_preprocessed'
    filtered_dir: 'ina_filtered'
    add_segment_metadata: true
    performance_measurement: true
    vad_engine: 'sm'
    overwrite: false
    save_payload: true
  pyannote_vad:
    output_dir: 'pyannote_vad_preprocessed'
    add_segment_metadata: true
    performance_measurement: true
    keep_only_first_segment: false
    model_params:
      onset: 0.85             # onset activation threshold
      offset: 0.8             # offset activation threshold
      min_duration_on: 0.1    # remove speech regions shorter than that many seconds
      min_duration_off: 0.0   # fill non-speech regions shorter than that many seconds
    overwrite: false
    save_payload: true
  pyannote_sd:  # pyannote speaker diarization, applies VAD and Embedding
    output_dir: 'pyannote_sd_preprocessed'
    classification_column_name: 'pyannote_diarization_classification'
    add_segment_metadata: true
    performance_measurement: true
    keep_only_first_segment: true
    skip_overlap: true
    overwrite: false
    save_payload: true
    save_payload_periodicity: 10000  # save intermediate payload results every X processed files
    hparams:
      pipeline:
        name: pyannote.audio.pipelines.SpeakerDiarization
        params:
          clustering: AgglomerativeClustering
          embedding: speechbrain/spkrec-ecapa-voxceleb
          embedding_batch_size: 32
          embedding_exclude_overlap: true
          segmentation: pyannote/segmentation@2022.07
          segmentation_batch_size: 32

      params:
        clustering:
          method: centroid  # ["average", "centroid", "complete", "median", "single", "ward", "weighted"]
          min_cluster_size: 13  # 15
          threshold: 0.71  # 0.7153814381597874
        segmentation:
          min_duration_off: 0.1  # 0.5817029604921046
          threshold: 0.6  # 0.4442333667381752
  silero_vad:
    output_dir: 'silero_vad_preprocessed'
    add_segment_metadata: true
    performance_measurement: true
    keep_only_first_segment: true
    max_workers: 1
    model_params:
      threshold: 0.8  # default 0.5. larger - more segmentized
#      sampling_rate: 16000,
      min_speech_duration_ms: 100  # default 250
#      max_speech_duration_s:
      min_silence_duration_ms: 80  # default 100
      window_size_samples: 512
      speech_pad_ms: 30
      return_seconds: False
    overwrite: false
    save_payload: true
    save_payload_periodicity: 50000  # save intermediate payload results every X processed files
  metricgan_se:  # MetricGAN+ speech enhancement component
    output_dir: 'metricgan_se_preprocessed'
    performance_measurement: true
    overwrite: false
    save_payload: true
  sepformer_se: # SepFormer model, implemented with SpeechBrain, and pretrained on WHAM! dataset
    output_dir: 'sepformer_se_preprocessed'
    performance_measurement: true
    overwrite: false
    save_payload: true

feature_extraction:
  librosa_features_extractor:
    performance_measurement: true
    n_mfcc: 13
    features:
      - mfcc
      - delta_mfcc
      - zero_crossing_rate
      - spectral_centroid
      - spectral_bandwidth
      - spectral_contrast
      - spectral_flatness
#      - f0
      - tonnetz
    save_payload: true
    save_payload_periodicity: 50000  # save intermediate payload results every X processed files
  pyannote_embedding:
    performance_measurement: true
    sliding_window_duration: 3.0
    sliding_window_step: 1.0
    save_payload: true
    save_payload_periodicity: 50000  # save intermediate payload results every X processed files
  speechbrain_embedding:
    performance_measurement: true
    save_payload: true
    save_payload_periodicity: 50000  # save intermediate payload results every X processed files
    model: spkrec-ecapa-voxceleb  # e.g. spkrec-ecapa-voxceleb, spkrec-xvect-voxceleb, ...
  vanpy_speaker_embedding:
    pretrained_models_dir: 'pretrained_models/vanpy_speaker_embedding'
    performance_measurement: true
    save_payload: true
    save_payload_periodicity: 50000  # save intermediate payload results every X processed files
#    model: 20230517-020537_loss_0.005514454140314478-ep_24-batch_128-lr_1e-06.pt

segment_classifier:
  vanpy_gender:
    pretrained_models_dir: 'pretrained_models/vanpy_gender'
    classification_column_name: 'vanpy_gender_classification'
    model: 'svm_ecapa_192_sb_voxceleb'  #  choose from 'svm_ecapa_192_sb_voxceleb', 'svm_xvect_512_sb_voxceleb'
    verbal_labels: true
    apply_transform: true  # run fitted standard-scaler on features
    performance_measurement: true
    save_payload: true

  vanpy_age:
    pretrained_models_dir: 'pretrained_models/vanpy_age'
    classification_column_name: 'vanpy_age_estimation'
    model: 'ann_ecapa_192_sb_librosa_31_combined'  #  choose from 'svr_ecapa_192_sb_voxceleb'
                                                   #              'svr_ecapa_192_sb_librosa_31_voxceleb'
                                                   #              'ann_ecapa_192_sb_timit'
                                                   #              'ann_ecapa_192_sb_librosa_31_combined'
    apply_transform: true  # run fitted standard-scaler on features, not inplace
    performance_measurement: true
    save_payload: true

  vanpy_height:
    pretrained_models_dir: 'pretrained_models/vanpy_height'
    classification_column_name: 'vanpy_height_estimation'
    apply_transform: true  # run fitted standard-scaler on features, not inplace
    performance_measurement: true
    save_payload: true

  vanpy_emotion:
    pretrained_models_dir: 'pretrained_models/vanpy_emotion'
    classification_column_name: 'vanpy_emotion_classification'
    verbal_labels: true
    save_payload: true

#  common_voices_gender:
#    pretrained_models_dir: 'pretrained_models/common_voice'
#    classification_column_name: 'common_voices_gender_classification'
#    verbal_labels: true
#    save_payload: true
#
#  common_voices_age:
#    pretrained_models_dir: 'pretrained_models/common_voice'
#    classification_column_name: 'common_voices_age_classification'
#    verbal_labels: true
#    save_payload: true

  speech_brain_iemocap_emotion:
    pretrained_models_dir: 'pretrained_models/speech_brain_iemocap_emotion'
    classification_column_name: 'speech_brain_iemocap_emotion'
    performance_measurement: true
    verbal_labels: true
    save_payload: true

  wav2vec2adv:
    pretrained_models_dir: 'pretrained_models/wav2vec2-large-robust-12-ft-emotion-msp-dim'
    performance_measurement: true
    save_payload: true

  wav2vec2stt:
    pretrained_models_dir: 'pretrained_models/wav2vec2'
    classification_column_name: 'wav2vec2_transcript'
    performance_measurement: true
    save_payload: true

  openai_whisper_stt:
    # Pay attention: "Inference is currently only implemented for short-form i.e. audio is pre-segmented into <=30s segments"
    max_workers: 1  # whisper doesn't support multi-threading, https://github.com/openai/whisper/discussions/951
    model_size: 'small'  # 'tiny', 'base', 'small', 'medium', 'large'
    pretrained_models_dir: 'pretrained_models/whisper'
    stt_column_name: 'whisper_transcript'
    detect_language: true
    language_classification_column_name: 'whisper_language'
    performance_measurement: true
    save_payload: true
    save_payload_periodicity: 10000

  cosine_distance_diarization:
    classification_column_name: 'cosine_distance_diarization'
    threshold: 0.3  # cosine similarity threshold of the embeddings
    features_list:  # assumes multiple column features are labeled 'i_<component_name>' where i is between
                    # start_index (inclusive) and stop_index (exclusive)
      - speechbrain_embedding:
          start_index: 0
          stop_index: 192
    performance_measurement: true
    save_payload: true

  agglomerative_clustering_diarization:
    classification_column_name: 'agglomerative_clustering_diarization'
    threshold: 2.3  # float or blank (None), set to None if using n_clusters
    n_clusters:  # int or blank (None), set to None if using threshold
    features_list: # assumes multiple column features are labeled 'i_<component_name>' where i is between
      # start_index (inclusive) and stop_index (exclusive)
      - speechbrain_embedding:
          start_index: 0
          stop_index: 192
    performance_measurement: true
    save_payload: true

  gmm_clustering_diarization:
    classification_column_name: 'gmm_clustering_diarization'
    n_components: 6
    covariance_type: 'full'  # ‘full’, ‘tied’, ‘diag’, ‘spherical’
    features_list: # assumes multiple column features are labeled 'i_<component_name>' where i is between
      # start_index (inclusive) and stop_index (exclusive)
      - speechbrain_embedding:
          start_index: 0
          stop_index: 192
    performance_measurement: true
    save_payload: true

  yamnet_classifier:
    pretrained_models_dir: 'pretrained_models/yamnet_classifier'
    classification_column_name: 'yamnet_classification'
    top_k: 3
    threshold: 0.7
    verbal_labels: true
    save_payload: true


