input_dir: 'speech_examples_small'
segment_name_separator: "_"
intermediate_payload_path: 'results'
# device: 'cpu'  # 'cpu'/'cuda'
log_each_x_records: 1000
huggingface_ACCESS_TOKEN: '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'
sampling_rate: 16000

preprocessing:
  file_mapper:
    save_payload: false  # use it if you want to save the payload when the component process is completed
    load_payload: false  # use it if you want to load a previously saved payload, overrides listing of an input_path
    load_df_path: '{{intermediate_payload_path}}/preprocessing_silero_vad_df_20221031175555_final.csv'
    load_meta_path: '{{intermediate_payload_path}}/preprocessing_silero_vad_metadata_20221031175555_final.pickle'
  wav_converter:
    output_dir: 'convert_preprocessed'
    ab: '256k'  # bitrate, may not be considered depending on a chosen codec
    ac: 1       # number of channels
    ar: '{{sampling_rate}}'   # sample frequency
    acodec: 'pcm_s16le'  # specify codec
    use_dir_name_as_prefix: true  # will name converted file with parent_dir prefix:
                                  # '<path>/<parent_dir>/<filename>' -> '<ouput_dir>/<parent_dir>_<filename>'
    overwrite: false
    save_payload: true
  ina_speech_segmenter:
    output_dir: 'ina_preprocessed'
    filtered_dir: 'ina_filtered'
    add_segment_metadata: true
    performance_measurement: true
    vad_engine: 'sm'
    overwrite: false
    save_payload: true
  pyannote_vad:
    output_dir: 'pyannote_vad_preprocessed'
    add_segment_metadata: true
    performance_measurement: true
    keep_only_first_segment: false
    model_params:
      onset: 0.85             # onset activation threshold
      offset: 0.8             # offset activation threshold
      min_duration_on: 0.1    # remove speech regions shorter than that many seconds
      min_duration_off: 0.0   # fill non-speech regions shorter than that many seconds
    overwrite: false
    save_payload: true
  pyannote_sd:  # pyannote speaker diarization, applies VAD and Embedding
    output_dir: 'pyannote_sd_preprocessed'
    classification_column_name: 'pyannote_diarization_classification'
    add_segment_metadata: true
    performance_measurement: true
    skip_overlap: true
    overwrite: true
    save_payload: true
  silero_vad:
    output_dir: 'silero_vad_preprocessed'
    add_segment_metadata: true
    performance_measurement: true
    keep_only_first_segment: true
    model_params:
      threshold: 0.8  # default 0.5. larger - more segmentized
#      sampling_rate: 16000,
      min_speech_duration_ms: 100  # default 250
#      max_speech_duration_s:
      min_silence_duration_ms: 80  # default 100
      window_size_samples: 512
      speech_pad_ms: 30
      return_seconds: False
    overwrite: false
    save_payload: true
    save_payload_periodicity: 50000  # save intermediate payload results every X processed files
  metricgan_se:  # MetricGAN+ speech enhancement component
    output_dir: 'metricgan_se_preprocessed'
    performance_measurement: true
    overwrite: false
    save_payload: true

feature_extraction:
  librosa_features_extractor:
    performance_measurement: true
    n_mfcc: 13
    features:
      - mfcc
      - delta_mfcc
      - zero_crossing_rate
      - spectral_centroid
      - spectral_bandwidth
      - spectral_contrast
#      - spectral_flatness
#      - f0
      - tonnetz
    save_payload: true
    save_payload_periodicity: 50000  # save intermediate payload results every X processed files
  pyannote_embedding:
    performance_measurement: true
    sliding_window_duration: 3.0
    sliding_window_step: 1.0
    save_payload: true
    save_payload_periodicity: 50000  # save intermediate payload results every X processed files
  speechbrain_embedding:
    performance_measurement: true
    save_payload: true
    save_payload_periodicity: 50000  # save intermediate payload results every X processed files
    model: spkrec-ecapa-voxceleb  # e.g. spkrec-ecapa-voxceleb, spkrec-xvect-voxceleb, ...

segment_classifier:
  vanpy_voxceleb_gender:
    pretrained_models_dir: 'pretrained_models/vanpy_voxceleb_gender'
    classification_column_name: 'vanpy_voxceleb_gender_classification'
    verbal_labels: true
    performance_measurement: true
    save_payload: true

  vanpy_voxceleb_age:
    pretrained_models_dir: 'pretrained_models/vanpy_voxceleb_age'
    classification_column_name: 'vanpy_voxceleb_age_estimation'
    performance_measurement: true
    save_payload: true

  vanpy_voxceleb_height:
    pretrained_models_dir: 'pretrained_models/vanpy_voxceleb_height'
    classification_column_name: 'vanpy_voxceleb_height_estimation'
    performance_measurement: true
    save_payload: true

  common_voices_gender:
    pretrained_models_dir: 'pretrained_models/common_voice'
    classification_column_name: 'common_voices_gender_classification'
    verbal_labels: true
    save_payload: true

  common_voices_age:
    pretrained_models_dir: 'pretrained_models/common_voice'
    classification_column_name: 'common_voices_age_classification'
    verbal_labels: true
    save_payload: true

  speech_brain_iemocap_emotion:
    pretrained_models_dir: 'pretrained_models/speech_brain_iemocap_emotion'
    classification_column_name: 'speech_brain_iemocap_emotion'
    verbal_labels: true
    save_payload: true

  vanpy_ravdess_emotion:
    pretrained_models_dir: 'pretrained_models/vanpy_ravdess_emotion'
    classification_column_name: 'vanpy_ravdess_emotion'
    verbal_labels: true
    save_payload: true

  wav2vec2stt:
    pretrained_models_dir: 'pretrained_models/wav2vec2'
    classification_column_name: 'wav2vec2_transcript'
    performance_measurement: true
    save_payload: true

  openai_whisper_stt:
    # Pay attention: "Inference is currently only implemented for short-form i.e. audio is pre-segmented into <=30s segments"
    model_size: 'tiny'  # 'tiny', 'base', 'small', 'medium', 'large'
    pretrained_models_dir: 'pretrained_models/whisper'
    stt_column_name: 'whisper_transcript'
    detect_language: true
    language_classification_column_name: 'whisper_language'
    performance_measurement: true
    save_payload: true

  cosine_distance_diarization:
    classification_column_name: 'diarization_classification'
    threshold: 0.25  # cosine similarity threshold of the embeddings
    features_list:  # assumes multiple column features are labeled 'i_<component_name>' where i is between
                    # start_index (inclusive) and stop_index (exclusive)
      - speechbrain_embedding:
          start_index: 0
          stop_index: 192
    performance_measurement: true
    save_payload: true

